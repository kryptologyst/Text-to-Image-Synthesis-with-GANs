# @package training

# Training settings
max_epochs: 100
gradient_clip_val: 1.0
accumulate_grad_batches: 1

# Optimizer settings
optimizer:
  generator:
    lr: 0.0002
    betas: [0.5, 0.999]
    weight_decay: 0.0
  discriminator:
    lr: 0.0002
    betas: [0.5, 0.999]
    weight_decay: 0.0

# Scheduler settings
scheduler:
  generator:
    _target_: torch.optim.lr_scheduler.CosineAnnealingLR
    T_max: 100
    eta_min: 0.00001
  discriminator:
    _target_: torch.optim.lr_scheduler.CosineAnnealingLR
    T_max: 100
    eta_min: 0.00001

# Loss settings
loss:
  generator_weight: 1.0
  discriminator_weight: 1.0
  gradient_penalty_weight: 10.0
  use_gradient_penalty: true

# Validation
val_check_interval: 1.0
limit_val_batches: 1.0
